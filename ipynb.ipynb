{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incident-criticism",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invalid-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.4.3\n",
      "  Downloading pandas-1.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.7 MB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==3.5.2\n",
      "  Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 25.2 MB/s eta 0:00:01     |██████████████████████████████▌ | 10.7 MB 25.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==1.1.1\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2 MB 26.0 MB/s eta 0:00:01    |███████                         | 6.9 MB 15.2 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 59 kB/s eta 0:00:0101\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (2.4.7)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
      "\u001b[K     |████████████████████████████████| 944 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (20.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (8.4.0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas==1.4.3->-r requirements.txt (line 1)) (2021.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 34.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 43.4 MB 23.2 MB/s eta 0:00:01    |███                             | 3.9 MB 28.6 MB/s eta 0:00:02     |███████████████████▌            | 26.4 MB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.5.2->-r requirements.txt (line 2)) (1.15.0)\n",
      "Installing collected packages: threadpoolctl, scipy, kiwisolver, joblib, fonttools, cycler, scikit-learn, pandas, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.34.4 joblib-1.1.0 kiwisolver-1.4.4 matplotlib-3.5.2 pandas-1.4.3 scikit-learn-1.1.1 scipy-1.9.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hispanic-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 95025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dense, Activation, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "seed = 0\n",
    "\n",
    "data = pd.read_csv('name_gender.csv')\n",
    "print(f'Size of dataset: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "former-today",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>95025</td>\n",
       "      <td>95025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>95025</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Aaban&amp;&amp;</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>60304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name gender\n",
       "count     95025  95025\n",
       "unique    95025      2\n",
       "top     Aaban&&      F\n",
       "freq          1  60304"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neural-candy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any() # No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "otherwise-right",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(data['name'].values).isascii() # No accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "central-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F    60304\n",
       "M    34721\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gender'].value_counts() # Class frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-solomon",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "different-alloy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            name gender\n",
      "0        Aaban&&      M\n",
      "1         Aabha*      F\n",
      "4          Aada_      F\n",
      "10       Aadhav+      M\n",
      "13      Aadhira4      F\n",
      "...          ...    ...\n",
      "94826   Zyair770      M\n",
      "94874  Zyheir887      M\n",
      "94915    Zykir24      M\n",
      "94957  Zymirah11      F\n",
      "94995     Zyri*&      F\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of non-alphabetic names\n",
    "print(data[data['name'].str.contains('\\W|\\d|_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-school",
   "metadata": {},
   "source": [
    "The following step removes non-alphabetic characters (numbers, symbols) from the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "local-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['name'] = data['name'].str.replace('\\W|\\d|_','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sustainable-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset stratified by gender before tokenizing to avoid leakage of test set info into training features\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['name'], data['gender'], test_size=0.1, \\\n",
    "                                                    random_state=seed, stratify=data['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-auditor",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-optimum",
   "metadata": {},
   "source": [
    "Various models will be tried. \n",
    "\n",
    "Additionally, each model will be trained to 2 different sets of input features. First, a set of ngrams will be created from each name using sklearn's count vectorizer, essentially a bag of words approach before applying a machine learning model. The second method will further convert the count vectors into TF-IDF vectors.\n",
    "\n",
    "Applying TF-IDF typically scales down the impact of tokens that are common in a corpus (hence less informative), although it is not clear that that might be the case for this problem. The higher the score, the more relevant that token is in that particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "timely-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 6\n"
     ]
    }
   ],
   "source": [
    "ngram_upper = int(np.floor(np.mean(data['name'].apply(len))))\n",
    "print(f'Average word length: {ngram_upper}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-saudi",
   "metadata": {},
   "source": [
    "A minimum of 2 characters and a maximum of 6 (mean word length) is used for constructing ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-monster",
   "metadata": {},
   "source": [
    "For hyerparameter tuning, stratified 5-fold CV will be used. However, tokenization will only be performed after splitting the dataset in order to prevent the validation set from having any info on the features of the other training folds. This procedure will be combined with gridsearchcv using a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-chorus",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-canon",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "still-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'logisticregression__C': 0.005}</td>\n",
       "      <td>0.865953</td>\n",
       "      <td>3</td>\n",
       "      <td>0.872653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'logisticregression__C': 0.006}</td>\n",
       "      <td>0.868595</td>\n",
       "      <td>2</td>\n",
       "      <td>0.875827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'logisticregression__C': 0.007}</td>\n",
       "      <td>0.870758</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             params  mean_test_score  rank_test_score  \\\n",
       "0  {'logisticregression__C': 0.005}         0.865953                3   \n",
       "1  {'logisticregression__C': 0.006}         0.868595                2   \n",
       "2  {'logisticregression__C': 0.007}         0.870758                1   \n",
       "\n",
       "   mean_train_score  \n",
       "0          0.872653  \n",
       "1          0.875827  \n",
       "2          0.878829  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tuning section was iteratively run to search for optimal hyperparameters without overfitting.\n",
    "# I avoided single comprehensive searches as my computer was overheating and crashing. \n",
    "# The results printed here are my final results\n",
    "\n",
    "# As GridsearchCV optimizes for the best validation score, it is likely to overfit since it ignores the training score \n",
    "# in its selection. The training scores are therefore printed and included in the manual selection of the best \n",
    "# hyperparameters to use\n",
    "\n",
    "# Pipeline for tokenization and modelling\n",
    "pipeline = make_pipeline(\n",
    "        CountVectorizer(analyzer='char_wb', ngram_range=(2,ngram_upper)),\n",
    "        LogisticRegression(max_iter=200)\n",
    "        )\n",
    "\n",
    "# Gridsearch with 5-fold CV\n",
    "parameters = {'logisticregression__C':[0.005,0.006,0.007]}\n",
    "lr_1 = GridSearchCV(pipeline, param_grid=parameters, cv=5, verbose=1, return_train_score=True) # Monitor for overfitting\n",
    "lr_1.fit(X_train, y_train)\n",
    "\n",
    "pd.DataFrame(lr_1.cv_results_)[['params','mean_test_score','rank_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "different-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for tuned LR model\n",
      "Train: 0.882\n",
      "Test: 0.876\n"
     ]
    }
   ],
   "source": [
    "print(f'Scores for tuned LR model\\nTrain: {round(lr_1.score(X_train, y_train),3)}\\nTest: {round(lr_1.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-animal",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "piano-private",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] END .........................logisticregression__C=0.13; total time=  10.2s\n",
      "[CV] END .........................logisticregression__C=0.13; total time=   9.2s\n",
      "[CV] END .........................logisticregression__C=0.13; total time=   9.0s\n",
      "[CV] END .........................logisticregression__C=0.13; total time=   9.6s\n",
      "[CV] END .........................logisticregression__C=0.13; total time=  10.5s\n",
      "[CV] END .........................logisticregression__C=0.15; total time=   8.4s\n",
      "[CV] END .........................logisticregression__C=0.15; total time=   7.9s\n",
      "[CV] END .........................logisticregression__C=0.15; total time=   9.3s\n",
      "[CV] END .........................logisticregression__C=0.15; total time=   7.9s\n",
      "[CV] END .........................logisticregression__C=0.15; total time=   9.7s\n",
      "[CV] END .........................logisticregression__C=0.18; total time=   9.3s\n",
      "[CV] END .........................logisticregression__C=0.18; total time=   8.0s\n",
      "[CV] END .........................logisticregression__C=0.18; total time=   8.4s\n",
      "[CV] END .........................logisticregression__C=0.18; total time=   9.0s\n",
      "[CV] END .........................logisticregression__C=0.18; total time=   8.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'logisticregression__C': 0.13}</td>\n",
       "      <td>0.864245</td>\n",
       "      <td>3</td>\n",
       "      <td>0.876953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'logisticregression__C': 0.15}</td>\n",
       "      <td>0.867613</td>\n",
       "      <td>2</td>\n",
       "      <td>0.880966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'logisticregression__C': 0.18}</td>\n",
       "      <td>0.871191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            params  mean_test_score  rank_test_score  \\\n",
       "0  {'logisticregression__C': 0.13}         0.864245                3   \n",
       "1  {'logisticregression__C': 0.15}         0.867613                2   \n",
       "2  {'logisticregression__C': 0.18}         0.871191                1   \n",
       "\n",
       "   mean_train_score  \n",
       "0          0.876953  \n",
       "1          0.880966  \n",
       "2          0.885871  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline using TF-IDF vectorizer\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "        TfidfVectorizer(analyzer='char_wb', ngram_range=(2,ngram_upper)),\n",
    "        LogisticRegression(max_iter=200)\n",
    "        )\n",
    "\n",
    "parameters = {'logisticregression__C':[0.13,0.15,0.18]}\n",
    "lr_2 = GridSearchCV(pipeline, param_grid=parameters, cv=5, verbose=2, return_train_score=True) # Monitor for overfitting\n",
    "lr_2.fit(X_train, y_train)\n",
    "pd.DataFrame(lr_2.cv_results_)[['params','mean_test_score','rank_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "residential-lyric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for tuned LR model\n",
      "Train: 0.889\n",
      "Test: 0.879\n"
     ]
    }
   ],
   "source": [
    "print(f'Scores for tuned LR model\\nTrain: {round(lr_2.score(X_train, y_train),3)}\\nTest: {round(lr_2.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-citizenship",
   "metadata": {},
   "source": [
    "## Multinomial NB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-approval",
   "metadata": {},
   "source": [
    "The code is identical to the ones above except for the different model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-divorce",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "selective-rapid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'multinomialnb__alpha': 4}</td>\n",
       "      <td>0.861848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.901291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        params  mean_test_score  rank_test_score  \\\n",
       "0  {'multinomialnb__alpha': 4}         0.861848                1   \n",
       "\n",
       "   mean_train_score  \n",
       "0          0.901291  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "        CountVectorizer(analyzer='char_wb', ngram_range=(2,ngram_upper)),\n",
    "        MultinomialNB()\n",
    "        )\n",
    "\n",
    "parameters = {'multinomialnb__alpha':[4]}\n",
    "nb_1 = GridSearchCV(pipeline, param_grid=parameters, cv=5, verbose=1, return_train_score=True) # Monitor for overfitting\n",
    "nb_1.fit(X_train, y_train)\n",
    "\n",
    "pd.DataFrame(nb_1.cv_results_)[['params','mean_test_score','rank_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "republican-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for tuned NB model\n",
      "Train: 0.902\n",
      "Test: 0.87\n"
     ]
    }
   ],
   "source": [
    "print(f'Scores for tuned NB model\\nTrain: {round(nb_1.score(X_train, y_train),3)}\\nTest: {round(nb_1.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-harvey",
   "metadata": {},
   "source": [
    "### tdidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fewer-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'multinomialnb__alpha': 2}</td>\n",
       "      <td>0.809371</td>\n",
       "      <td>1</td>\n",
       "      <td>0.856192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'multinomialnb__alpha': 3}</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>2</td>\n",
       "      <td>0.819184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'multinomialnb__alpha': 5}</td>\n",
       "      <td>0.748556</td>\n",
       "      <td>3</td>\n",
       "      <td>0.770673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        params  mean_test_score  rank_test_score  \\\n",
       "0  {'multinomialnb__alpha': 2}         0.809371                1   \n",
       "1  {'multinomialnb__alpha': 3}         0.784091                2   \n",
       "2  {'multinomialnb__alpha': 5}         0.748556                3   \n",
       "\n",
       "   mean_train_score  \n",
       "0          0.856192  \n",
       "1          0.819184  \n",
       "2          0.770673  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "        TfidfVectorizer(analyzer='char_wb', ngram_range=(2,ngram_upper)),\n",
    "        MultinomialNB()\n",
    "        )\n",
    "\n",
    "parameters = {'multinomialnb__alpha':[2,3,5]}\n",
    "nb_2 = GridSearchCV(pipeline, param_grid=parameters, cv=5, verbose=1, return_train_score=True) # Monitor for overfitting\n",
    "nb_2.fit(X_train, y_train)\n",
    "\n",
    "pd.DataFrame(nb_2.cv_results_)[['params','mean_test_score','rank_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "exempt-exhibit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for tuned NB model\n",
      "Train: 0.862\n",
      "Test: 0.821\n"
     ]
    }
   ],
   "source": [
    "print(f'Scores for tuned NB model\\nTrain: {round(nb_2.score(X_train, y_train),3)}\\nTest: {round(nb_2.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-buddy",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-florist",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "earned-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fail to converge, not scalable method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "amazing-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................................linearsvc__C=5; total time= 1.4min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-63d1d44c5a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'linearsvc__C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msvc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Monitor for overfitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msvc_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rank_test_score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean_train_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    820\u001b[0m                     )\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    823\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0m\u001b[1;32m   1206\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "        CountVectorizer(analyzer='char_wb', ngram_range=(2,ngram_upper)),\n",
    "        StandardScaler(with_mean=False),\n",
    "        LinearSVC(max_iter=5000)\n",
    "        )\n",
    "\n",
    "parameters = {'linearsvc__C':[5]}\n",
    "svc_1 = GridSearchCV(pipeline, param_grid=parameters, cv=5, verbose=2, return_train_score=True) # Monitor for overfitting\n",
    "svc_1.fit(X_train, y_train)\n",
    "\n",
    "pd.DataFrame(svc_1.cv_results_)[['params','mean_test_score','rank_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-azerbaijan",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-coverage",
   "metadata": {},
   "source": [
    "A separate procedure for data preprocessing is used for training a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "electric-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['name'] = data['name'].str.lower()\n",
    "\n",
    "# Construct a vocabulary including 'END' token\n",
    "vocab = set(''.join(data['name']))\n",
    "vocab.add('END')\n",
    "\n",
    "# Mapping characters to indices\n",
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bulgarian-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing X and y datasets for use by LSTM\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "maxlen = 20\n",
    "\n",
    "# Builds an empty line with a 1 at the index of character\n",
    "def set_flag(i):\n",
    "    tmp = np.zeros(len(vocab));\n",
    "    tmp[i] = 1\n",
    "    return list(tmp)\n",
    "\n",
    "# Truncate names and create the matrix\n",
    "def prepare_X(X):\n",
    "    new_list = []\n",
    "    trunc_train_name = [str(i)[0:maxlen] for i in X]\n",
    "\n",
    "    for i in trunc_train_name:\n",
    "        tmp = [set_flag(char_index[j]) for j in str(i)]\n",
    "        for k in range(0,maxlen - len(str(i))):\n",
    "            tmp.append(set_flag(char_index[\"END\"]))\n",
    "        new_list.append(tmp)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "\n",
    "X = prepare_X(data['name'].values)\n",
    "\n",
    "# Label Encoding of y\n",
    "def prepare_y(y):\n",
    "    new_list = []\n",
    "    for i in y:\n",
    "        if i == 'M':\n",
    "            new_list.append([1,0])\n",
    "        else:\n",
    "            new_list.append([0,1])\n",
    "\n",
    "    return new_list\n",
    "\n",
    "y = prepare_y(data['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "laden-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=0.1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "recent-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(512, return_sequences=True), backward_layer=LSTM(512, return_sequences=True, go_backwards=True), input_shape=(maxlen,len(vocab))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(512)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activity_regularizer=l2(0.002)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "major-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 20, 1024)         2211840   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 1024)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 1024)             6295552   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 2050      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,509,442\n",
      "Trainable params: 8,509,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "continuing-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "mc = ModelCheckpoint('lstm/model{epoch:08d}.h5', monitor='val_loss', mode='min', verbose=1)\n",
    "reduce_lr_acc = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, verbose=1, min_delta=1e-4, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "furnished-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.4805 - accuracy: 0.7731\n",
      "Epoch 00001: saving model to lstm/model00000001.h5\n",
      "168/168 [==============================] - 673s 4s/step - loss: 0.4805 - accuracy: 0.7731 - val_loss: 0.4145 - val_accuracy: 0.8229 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8341\n",
      "Epoch 00002: saving model to lstm/model00000002.h5\n",
      "168/168 [==============================] - 673s 4s/step - loss: 0.3825 - accuracy: 0.8341 - val_loss: 0.3623 - val_accuracy: 0.8475 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.8486\n",
      "Epoch 00003: saving model to lstm/model00000003.h5\n",
      "168/168 [==============================] - 629s 4s/step - loss: 0.3541 - accuracy: 0.8486 - val_loss: 0.3562 - val_accuracy: 0.8477 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.8563\n",
      "Epoch 00004: saving model to lstm/model00000004.h5\n",
      "168/168 [==============================] - 636s 4s/step - loss: 0.3392 - accuracy: 0.8563 - val_loss: 0.3632 - val_accuracy: 0.8445 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.3215 - accuracy: 0.8653\n",
      "Epoch 00005: saving model to lstm/model00000005.h5\n",
      "168/168 [==============================] - 614s 4s/step - loss: 0.3215 - accuracy: 0.8653 - val_loss: 0.3052 - val_accuracy: 0.8693 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8746\n",
      "Epoch 00006: saving model to lstm/model00000006.h5\n",
      "168/168 [==============================] - 601s 4s/step - loss: 0.3024 - accuracy: 0.8746 - val_loss: 0.2968 - val_accuracy: 0.8782 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8813\n",
      "Epoch 00007: saving model to lstm/model00000007.h5\n",
      "168/168 [==============================] - 606s 4s/step - loss: 0.2891 - accuracy: 0.8813 - val_loss: 0.2967 - val_accuracy: 0.8798 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.8870\n",
      "Epoch 00008: saving model to lstm/model00000008.h5\n",
      "168/168 [==============================] - 613s 4s/step - loss: 0.2792 - accuracy: 0.8870 - val_loss: 0.2848 - val_accuracy: 0.8852 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.8950\n",
      "Epoch 00009: saving model to lstm/model00000009.h5\n",
      "168/168 [==============================] - 608s 4s/step - loss: 0.2640 - accuracy: 0.8950 - val_loss: 0.2855 - val_accuracy: 0.8842 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9001\n",
      "Epoch 00010: saving model to lstm/model00000010.h5\n",
      "168/168 [==============================] - 612s 4s/step - loss: 0.2544 - accuracy: 0.9001 - val_loss: 0.2665 - val_accuracy: 0.8929 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=10, verbose=1, validation_data =(X_test, y_test), callbacks=[callback, mc, reduce_lr_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "thirty-spell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adjustable-kinase",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f8d67473790> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f8ce0cbac70> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f8d674dfd60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f8d674f5730> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "billion-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-celebrity",
   "metadata": {},
   "source": [
    "# App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "liable-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow.keras.models.load_model('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "durable-fleece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00608571, 0.9939143 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "about-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = prepare_X(['aabriella'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "honest-progress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_y(['F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "approved-pressure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
