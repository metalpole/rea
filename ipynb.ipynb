{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incident-criticism",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invalid-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.4.3\n",
      "  Downloading pandas-1.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.7 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==3.5.2\n",
      "  Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==1.1.1\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2 MB 35.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (2.4.7)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
      "\u001b[K     |████████████████████████████████| 944 kB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas==1.4.3->-r requirements.txt (line 1)) (2021.1)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 40.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 43.4 MB 4.0 MB/s eta 0:00:011     |███████████████▊                | 21.3 MB 44.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.5.2->-r requirements.txt (line 2)) (1.15.0)\n",
      "Installing collected packages: threadpoolctl, scipy, kiwisolver, joblib, fonttools, cycler, scikit-learn, pandas, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.34.4 joblib-1.1.0 kiwisolver-1.4.4 matplotlib-3.5.2 pandas-1.4.3 scikit-learn-1.1.1 scipy-1.9.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "hispanic-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 95025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seed = 0\n",
    "\n",
    "data = pd.read_csv('name_gender.csv')\n",
    "print(f'Size of dataset: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "former-today",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>95025</td>\n",
       "      <td>95025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>95025</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Aaban&amp;&amp;</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>60304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name gender\n",
       "count     95025  95025\n",
       "unique    95025      2\n",
       "top     Aaban&&      F\n",
       "freq          1  60304"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "neural-candy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "central-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F    60304\n",
       "M    34721\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-solomon",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-school",
   "metadata": {},
   "source": [
    "Remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "different-alloy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            name gender\n",
      "0        Aaban&&      M\n",
      "1         Aabha*      F\n",
      "4          Aada_      F\n",
      "10       Aadhav+      M\n",
      "13      Aadhira4      F\n",
      "...          ...    ...\n",
      "94826   Zyair770      M\n",
      "94874  Zyheir887      M\n",
      "94915    Zykir24      M\n",
      "94957  Zymirah11      F\n",
      "94995     Zyri*&      F\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "names = data['name'].str.contains('\\W|\\d|_').values.sum()\n",
    "\n",
    "# List of non-alphabetic names\n",
    "print(data[data['name'].str.contains('\\W|\\d|_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "local-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['name'] = data['name'].str.replace('\\W|\\d|_','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "hired-adaptation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F    60304\n",
       "M    34721\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "sustainable-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset stratified by gender before tokenizing to avoid leakage of test set info into training features\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['name'], data['gender'], test_size=0.1, random_state=seed, stratify=data['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-auditor",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-analyst",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2102.03692.pdf\n",
    "\n",
    "What’s in a Name? – Gender Classification of Names with\n",
    "Character Based Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sublime-disease",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-9ec9268595f4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-9ec9268595f4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    error analysis on names based on frequency\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "error analysis on names based on frequency\n",
    "maybe upsample rarer names on training set, keep test set the same\n",
    "\n",
    "naive bayes\n",
    "random forest\n",
    "lstm\n",
    "char-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-chorus",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-attack",
   "metadata": {},
   "source": [
    "Different features\n",
    "1) BOW of ngrams\n",
    "\n",
    "2) TF-IDF of ngrams\n",
    "scale down the impact of tokens that are common in a corpus and hence less informative (does it apply here?)\n",
    "\n",
    "3) Class scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "timely-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 6\n"
     ]
    }
   ],
   "source": [
    "ngram_upper = int(np.floor(np.mean(data['name'].apply(len))))\n",
    "print(f'Average word length: {ngram_upper}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-saudi",
   "metadata": {},
   "source": [
    "A minimum of 2 characters and a maximum of 6 (mean word length) is used for constructing ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-controversy",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-monster",
   "metadata": {},
   "source": [
    "For hyerparameter tuning, stratified 5-fold CV will be used. However, tokenization will only be performed after splitting the dataset in order to prevent the validation set from having any info on the features from the other folds. This procedure will be combined with gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] END ..........................logisticregression__C=0.8; total time=  17.1s\n"
     ]
    }
   ],
   "source": [
    "# 5 fold CV\n",
    "# Tokenization\n",
    "# Grid search\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "        CountVectorizer(analyzer='char_wb', ngram_range=(2,ngram_upper)),\n",
    "        LogisticRegression(max_iter=300)\n",
    "        )\n",
    "\n",
    "parameters = {'logisticregression__C':[0.8, 1, 2]}\n",
    "lr = GridSearchCV(pipeline, param_grid=parameters, cv=5, verbose=2)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# counts = ngram_vectorizer.fit_transform(test['name'])\n",
    "# counts.toarray().astype(int)\n",
    "# ngram_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "removed-development",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 1}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-animal",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-private",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
